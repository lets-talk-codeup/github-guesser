[
 {
  "repo": "microsoft/vcpkg",
  "language": "CMake",
  "readme_contents": "# Vcpkg: Overview\n\n[\u4e2d\u6587\u603b\u89c8](README_zh_CN.md)\n[Espa\u00f1ol](README_es.md)\n[\ud55c\uad6d\uc5b4](README_ko_KR.md)\n[Fran\u00e7ais](README_fr.md)\n\nVcpkg helps you manage C and C++ libraries on Windows, Linux and MacOS.\nThis tool and ecosystem are constantly evolving, and we always appreciate contributions!\n\nIf you've never used vcpkg before, or if you're trying to figure out how to use vcpkg,\ncheck out our [Getting Started](#getting-started) section for how to start using vcpkg.\n\nFor short description of available commands, once you've installed vcpkg,\nyou can run `vcpkg help`, or `vcpkg help [command]` for command-specific help.\n\n* Github: [https://github.com/microsoft/vcpkg](https://github.com/microsoft/vcpkg)\n* Slack: [https://cppalliance.org/slack/](https://cppalliance.org/slack/), the #vcpkg channel\n* Discord: [\\#include \\<C++\\>](https://www.includecpp.org), the #\ud83c\udf0fvcpkg channel\n* Docs: [Documentation](docs/README.md)\n\n[![Build Status](https://dev.azure.com/vcpkg/public/_apis/build/status/microsoft.vcpkg.ci?branchName=master)](https://dev.azure.com/vcpkg/public/_build/latest?definitionId=29&branchName=master)\n\n# Table of Contents\n\n- [Vcpkg: Overview](#vcpkg-overview)\n- [Table of Contents](#table-of-contents)\n- [Getting Started](#getting-started)\n  - [Quick Start: Windows](#quick-start-windows)\n  - [Quick Start: Unix](#quick-start-unix)\n  - [Installing Linux Developer Tools](#installing-linux-developer-tools)\n  - [Installing macOS Developer Tools](#installing-macos-developer-tools)\n    - [Installing GCC for macOS before 10.15](#installing-gcc-for-macos-before-1015)\n  - [Using vcpkg with CMake](#using-vcpkg-with-cmake)\n    - [Visual Studio Code with CMake Tools](#visual-studio-code-with-cmake-tools)\n    - [Vcpkg with Visual Studio CMake Projects](#vcpkg-with-visual-studio-cmake-projects)\n    - [Vcpkg with CLion](#vcpkg-with-clion)\n    - [Vcpkg as a Submodule](#vcpkg-as-a-submodule)\n- [Tab-Completion/Auto-Completion](#tab-completionauto-completion)\n- [Examples](#examples)\n- [Contributing](#contributing)\n- [License](#license)\n- [Telemetry](#telemetry)\n\n# Getting Started\n\nFirst, follow the quick start guide for either\n[Windows](#quick-start-windows), or [macOS and Linux](#quick-start-unix),\ndepending on what you're using.\n\nFor more information, see [Installing and Using Packages][getting-started:using-a-package].\nIf a library you need is not present in the vcpkg catalog,\nyou can [open an issue on the GitHub repo][contributing:submit-issue]\nwhere the vcpkg team and community can see it,\nand potentially add the port to vcpkg.\n\nAfter you've gotten vcpkg installed and working,\nyou may wish to add [tab completion](#tab-completionauto-completion) to your shell.\n\nFinally, if you're interested in the future of vcpkg,\ncheck out the [manifest][getting-started:manifest-spec] guide!\nThis is an experimental feature and will likely have bugs,\nso try it out and [open all the issues][contributing:submit-issue]!\n\n## Quick Start: Windows\n\nPrerequisites:\n- Windows 7 or newer\n- [Git][getting-started:git]\n- [Visual Studio][getting-started:visual-studio] 2015 Update 3 or greater with the English language pack\n\nFirst, download and bootstrap vcpkg itself; it can be installed anywhere,\nbut generally we recommend using vcpkg as a submodule for CMake projects,\nand installing it globally for Visual Studio projects.\nWe recommend somewhere like `C:\\src\\vcpkg` or `C:\\dev\\vcpkg`,\nsince otherwise you may run into path issues for some port build systems.\n\n```cmd\n> git clone https://github.com/microsoft/vcpkg\n> .\\vcpkg\\bootstrap-vcpkg.bat\n```\n\nTo install the libraries for your project, run:\n\n```cmd\n> .\\vcpkg\\vcpkg install [packages to install]\n```\n\nNote: This will install x86 libraries by default. To install x64, run:\n\n```cmd\n> .\\vcpkg\\vcpkg install [package name]:x64-windows\n```\n\nOr\n\n```cmd\n> .\\vcpkg\\vcpkg install [packages to install] --triplet=x64-windows\n```\n\nYou can also search for the libraries you need with the `search` subcommand:\n\n```cmd\n> .\\vcpkg\\vcpkg search [search term]\n```\n\nIn order to use vcpkg with Visual Studio,\nrun the following command (may require administrator elevation):\n\n```cmd\n> .\\vcpkg\\vcpkg integrate install\n```\n\nAfter this, you can now create a New non-CMake Project (or open an existing one).\nAll installed libraries are immediately ready to be `#include`'d and used\nin your project without additional configuration.\n\nIf you're using CMake with Visual Studio,\ncontinue [here](#vcpkg-with-visual-studio-cmake-projects).\n\nIn order to use vcpkg with CMake outside of an IDE,\nyou can use the toolchain file:\n\n```cmd\n> cmake -B [build directory] -S . -DCMAKE_TOOLCHAIN_FILE=[path to vcpkg]/scripts/buildsystems/vcpkg.cmake\n> cmake --build [build directory]\n```\n\nWith CMake, you will still need to `find_package` and the like to use the libraries.\nCheck out the [CMake section](#using-vcpkg-with-cmake) for more information,\nincluding on using CMake with an IDE.\n\nFor any other tools, including Visual Studio Code,\ncheck out the [integration guide][getting-started:integration].\n\n## Quick Start: Unix\n\nPrerequisites for Linux:\n- [Git][getting-started:git]\n- [g++][getting-started:linux-gcc] >= 6\n\nPrerequisites for macOS:\n- [Apple Developer Tools][getting-started:macos-dev-tools]\n- On macOS 10.14 or below, you will also need:\n  - [Homebrew][getting-started:macos-brew]\n  - [g++][getting-started:macos-gcc] >= 6 from Homebrew\n\nFirst, download and bootstrap vcpkg itself; it can be installed anywhere,\nbut generally we recommend using vcpkg as a submodule for CMake projects.\n\n```sh\n$ git clone https://github.com/microsoft/vcpkg\n$ ./vcpkg/bootstrap-vcpkg.sh\n```\n\nTo install the libraries for your project, run:\n\n```sh\n$ ./vcpkg/vcpkg install [packages to install]\n```\n\nYou can also search for the libraries you need with the `search` subcommand:\n\n```sh\n$ ./vcpkg/vcpkg search [search term]\n```\n\nIn order to use vcpkg with CMake, you can use the toolchain file:\n\n```sh\n$ cmake -B [build directory] -S . -DCMAKE_TOOLCHAIN_FILE=[path to vcpkg]/scripts/buildsystems/vcpkg.cmake\n$ cmake --build [build directory]\n```\n\nWith CMake, you will still need to `find_package` and the like to use the libraries.\nCheck out the [CMake section](#using-vcpkg-with-cmake)\nfor more information on how best to use vcpkg with CMake,\nand CMake Tools for VSCode.\n\nFor any other tools, check out the [integration guide][getting-started:integration].\n\n## Installing Linux Developer Tools\n\nAcross the different distros of Linux, there are different packages you'll\nneed to install:\n\n- Debian, Ubuntu, popOS, and other Debian-based distributions:\n\n```sh\n$ sudo apt-get update\n$ sudo apt-get install build-essential tar curl zip unzip\n```\n\n- CentOS\n\n```sh\n$ sudo yum install centos-release-scl\n$ sudo yum install devtoolset-7\n$ scl enable devtoolset-7 bash\n```\n\nFor any other distributions, make sure you're installing g++ 6 or above.\nIf you want to add instructions for your specific distro,\n[please open a PR][contributing:submit-pr]!\n\n## Installing macOS Developer Tools\n\nOn macOS 10.15, the only thing you should need to do is run the following in your terminal:\n\n```sh\n$ xcode-select --install\n```\n\nThen follow along with the prompts in the windows that comes up.\n\nOn macOS 10.14 and previous, you'll also need to install g++ from homebrew;\nfollow the instructions in the following section.\n\n### Installing GCC for macOS before 10.15\n\nThis will _only_ be necessary if you're using a macOS version from before 10.15.\nInstalling homebrew should be very easy; check out <brew.sh> for more information,\nbut at its simplest, run the following command:\n\n```sh\n$ /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\"\n```\n\nThen, in order to grab an up-to-date version of gcc, run the following:\n\n```sh\n$ brew install gcc\n```\n\nYou'll then be able to bootstrap vcpkg along with the [quick start guide](#quick-start-unix)\n\n## Using vcpkg with CMake\n\nIf you're using vcpkg with CMake, the following may help!\n\n### Visual Studio Code with CMake Tools\n\nAdding the following to your workspace `settings.json` will make\nCMake Tools automatically use vcpkg for libraries:\n\n```json\n{\n  \"cmake.configureSettings\": {\n    \"CMAKE_TOOLCHAIN_FILE\": \"[vcpkg root]/scripts/buildsystems/vcpkg.cmake\"\n  }\n}\n```\n\n### Vcpkg with Visual Studio CMake Projects\n\nOpen the CMake Settings Editor, and under `CMake toolchain file`,\nadd the path to the vcpkg toolchain file:\n\n```\n[vcpkg root]/scripts/buildsystems/vcpkg.cmake\n```\n\n### Vcpkg with CLion\n\nOpen the Toolchains settings\n(File > Settings on Windows and Linux, CLion > Preferences on macOS),\nand go to the CMake settings (Build, Execution, Deployment > CMake).\nFinally, in `CMake options`, add the following line:\n\n```\n-DCMAKE_TOOLCHAIN_FILE=[vcpkg root]/scripts/buildsystems/vcpkg.cmake\n```\n\nUnfortunately, you'll have to add this to each profile.\n\n### Vcpkg as a Submodule\n\nWhen using vcpkg as a submodule of your project,\nyou can add the following to your CMakeLists.txt before the first `project()` call,\ninstead of passing `CMAKE_TOOLCHAIN_FILE` to the cmake invocation.\n\n```cmake\nset(CMAKE_TOOLCHAIN_FILE ${CMAKE_CURRENT_SOURCE_DIR}/vcpkg/scripts/buildsystems/vcpkg.cmake\n  CACHE STRING \"Vcpkg toolchain file\")\n```\n\nThis will still allow people to not use vcpkg,\nby passing the `CMAKE_TOOLCHAIN_FILE` directly,\nbut it will make the configure-build step slightly easier.\n\n[getting-started:using-a-package]: docs/examples/installing-and-using-packages.md\n[getting-started:integration]: docs/users/integration.md\n[getting-started:git]: https://git-scm.com/downloads\n[getting-started:cmake-tools]: https://marketplace.visualstudio.com/items?itemName=ms-vscode.cmake-tools\n[getting-started:linux-gcc]: #installing-linux-developer-tools\n[getting-started:macos-dev-tools]: #installing-macos-developer-tools\n[getting-started:macos-brew]: #installing-gcc-on-macos\n[getting-started:macos-gcc]: #installing-gcc-on-macos\n[getting-started:visual-studio]: https://visualstudio.microsoft.com/\n[getting-started:manifest-spec]: docs/specifications/manifests.md\n\n# Tab-Completion/Auto-Completion\n\n`vcpkg` supports auto-completion of commands, package names,\nand options in both powershell and bash.\nTo enable tab-completion in the shell of your choice, run:\n\n```pwsh\n> .\\vcpkg integrate powershell\n```\n\nor\n\n```sh\n$ ./vcpkg integrate bash\n```\n\ndepending on the shell you use, then restart your console.\n\n# Examples\n\nSee the [documentation](docs/README.md) for specific walkthroughs,\nincluding [installing and using a package](docs/examples/installing-and-using-packages.md),\n[adding a new package from a zipfile](docs/examples/packaging-zipfiles.md),\nand [adding a new package from a GitHub repo](docs/examples/packaging-github-repos.md).\n\nOur docs are now also available online at our website https://vcpkg.io/. We really appreciate any and all feedback! You can submit an issue in https://github.com/vcpkg/vcpkg.github.io/issues.\n\nSee a 4 minute [video demo](https://www.youtube.com/watch?v=y41WFKbQFTw).\n\n# Contributing\n\nVcpkg is an open source project, and is thus built with your contributions.\nHere are some ways you can contribute:\n\n* [Submit Issues][contributing:submit-issue] in vcpkg or existing packages\n* [Submit Fixes and New Packages][contributing:submit-pr]\n\nPlease refer to our [Contributing Guide](CONTRIBUTING.md) for more details.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct][contributing:coc].\nFor more information see the [Code of Conduct FAQ][contributing:coc-faq]\nor email [opencode@microsoft.com](mailto:opencode@microsoft.com)\nwith any additional questions or comments.\n\n[contributing:submit-issue]: https://github.com/microsoft/vcpkg/issues/new/choose\n[contributing:submit-pr]: https://github.com/microsoft/vcpkg/pulls\n[contributing:coc]: https://opensource.microsoft.com/codeofconduct/\n[contributing:coc-faq]: https://opensource.microsoft.com/codeofconduct/\n\n# License\n\nThe code in this repository is licensed under the [MIT License](LICENSE.txt).\n\n# Telemetry\n\nvcpkg collects usage data in order to help us improve your experience.\nThe data collected by Microsoft is anonymous.\nYou can opt-out of telemetry by\n- running the bootstrap-vcpkg script with -disableMetrics\n- passing --disable-metrics to vcpkg on the command line\n- setting the VCPKG_DISABLE_METRICS environment variable\n\nRead more about vcpkg telemetry at docs/about/privacy.md\n"
 },
 {
  "repo": "microsoft/LightGBM",
  "language": "C++",
  "readme_contents": "<img src=https://github.com/microsoft/LightGBM/blob/master/docs/logo/LightGBM_logo_black_text.svg width=300 />\n\nLight Gradient Boosting Machine\n===============================\n\n[![Python-package GitHub Actions Build Status](https://github.com/microsoft/LightGBM/workflows/Python-package/badge.svg?branch=master)](https://github.com/microsoft/LightGBM/actions)\n[![R-package GitHub Actions Build Status](https://github.com/microsoft/LightGBM/workflows/R-package/badge.svg?branch=master)](https://github.com/microsoft/LightGBM/actions)\n[![CUDA Version GitHub Actions Build Status](https://github.com/microsoft/LightGBM/workflows/CUDA%20Version/badge.svg?branch=master)](https://github.com/microsoft/LightGBM/actions)\n[![Static Analysis GitHub Actions Build Status](https://github.com/microsoft/LightGBM/workflows/Static%20Analysis/badge.svg?branch=master)](https://github.com/microsoft/LightGBM/actions)\n[![Azure Pipelines Build Status](https://lightgbm-ci.visualstudio.com/lightgbm-ci/_apis/build/status/Microsoft.LightGBM?branchName=master)](https://lightgbm-ci.visualstudio.com/lightgbm-ci/_build/latest?definitionId=1)\n[![Appveyor Build Status](https://ci.appveyor.com/api/projects/status/1ys5ot401m0fep6l/branch/master?svg=true)](https://ci.appveyor.com/project/guolinke/lightgbm/branch/master)\n[![Documentation Status](https://readthedocs.org/projects/lightgbm/badge/?version=latest)](https://lightgbm.readthedocs.io/)\n[![Link checks](https://github.com/microsoft/LightGBM/workflows/Link%20checks/badge.svg)](https://github.com/microsoft/LightGBM/actions?query=workflow%3A%22Link+checks%22)\n[![License](https://img.shields.io/github/license/microsoft/lightgbm.svg)](https://github.com/microsoft/LightGBM/blob/master/LICENSE)\n[![Python Versions](https://img.shields.io/pypi/pyversions/lightgbm.svg?logo=python&logoColor=white)](https://pypi.org/project/lightgbm)\n[![PyPI Version](https://img.shields.io/pypi/v/lightgbm.svg?logo=pypi&logoColor=white)](https://pypi.org/project/lightgbm)\n[![CRAN Version](https://www.r-pkg.org/badges/version/lightgbm)](https://cran.r-project.org/package=lightgbm)\n\nLightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n\n- Faster training speed and higher efficiency.\n- Lower memory usage.\n- Better accuracy.\n- Support of parallel, distributed, and GPU learning.\n- Capable of handling large-scale data.\n\nFor further details, please refer to [Features](https://github.com/microsoft/LightGBM/blob/master/docs/Features.rst).\n\nBenefiting from these advantages, LightGBM is being widely-used in many [winning solutions](https://github.com/microsoft/LightGBM/blob/master/examples/README.md#machine-learning-challenge-winning-solutions) of machine learning competitions.\n\n[Comparison experiments](https://github.com/microsoft/LightGBM/blob/master/docs/Experiments.rst#comparison-experiment) on public datasets show that LightGBM can outperform existing boosting frameworks on both efficiency and accuracy, with significantly lower memory consumption. What's more, [distributed learning experiments](https://github.com/microsoft/LightGBM/blob/master/docs/Experiments.rst#parallel-experiment) show that LightGBM can achieve a linear speed-up by using multiple machines for training in specific settings.\n\nGet Started and Documentation\n-----------------------------\n\nOur primary documentation is at https://lightgbm.readthedocs.io/ and is generated from this repository. If you are new to LightGBM, follow [the installation instructions](https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html) on that site.\n\nNext you may want to read:\n\n- [**Examples**](https://github.com/microsoft/LightGBM/tree/master/examples) showing command line usage of common tasks.\n- [**Features**](https://github.com/microsoft/LightGBM/blob/master/docs/Features.rst) and algorithms supported by LightGBM.\n- [**Parameters**](https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst) is an exhaustive list of customization you can make.\n- [**Distributed Learning**](https://github.com/microsoft/LightGBM/blob/master/docs/Parallel-Learning-Guide.rst) and [**GPU Learning**](https://github.com/microsoft/LightGBM/blob/master/docs/GPU-Tutorial.rst) can speed up computation.\n- [**Laurae++ interactive documentation**](https://sites.google.com/view/lauraepp/parameters) is a detailed guide for hyperparameters.\n- [**FLAML**](https://www.microsoft.com/en-us/research/project/fast-and-lightweight-automl-for-large-scale-data/articles/flaml-a-fast-and-lightweight-automl-library/) provides automated tuning for LightGBM ([code examples](https://github.com/microsoft/FLAML/blob/main/notebook/flaml_lightgbm.ipynb)).\n- [**Optuna Hyperparameter Tuner**](https://medium.com/optuna/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258) provides automated tuning for LightGBM hyperparameters ([code examples](https://github.com/optuna/optuna/tree/master/examples/lightgbm)).\n- [**Understanding LightGBM Parameters (and How to Tune Them using Neptune)**](https://neptune.ai/blog/lightgbm-parameters-guide).\n\nDocumentation for contributors:\n\n- [**How we update readthedocs.io**](https://github.com/microsoft/LightGBM/blob/master/docs/README.rst).\n- Check out the [**Development Guide**](https://github.com/microsoft/LightGBM/blob/master/docs/Development-Guide.rst).\n\nNews\n----\n\nPlease refer to changelogs at [GitHub releases](https://github.com/microsoft/LightGBM/releases) page.\n\nSome old update logs are available at [Key Events](https://github.com/microsoft/LightGBM/blob/master/docs/Key-Events.md) page.\n\nExternal (Unofficial) Repositories\n----------------------------------\n\nFLAML (AutoML library for hyperparameter optimization): https://github.com/microsoft/FLAML\n\nOptuna (hyperparameter optimization framework): https://github.com/optuna/optuna\n\nJulia-package: https://github.com/IQVIA-ML/LightGBM.jl\n\nJPMML (Java PMML converter): https://github.com/jpmml/jpmml-lightgbm\n\nTreelite (model compiler for efficient deployment): https://github.com/dmlc/treelite\n\nlleaves (LLVM-based model compiler for efficient inference): https://github.com/siboehm/lleaves\n\nHummingbird (model compiler into tensor computations): https://github.com/microsoft/hummingbird\n\ncuML Forest Inference Library (GPU-accelerated inference): https://github.com/rapidsai/cuml\n\ndaal4py (Intel CPU-accelerated inference): https://github.com/intel/scikit-learn-intelex/tree/master/daal4py\n\nm2cgen (model appliers for various languages): https://github.com/BayesWitnesses/m2cgen\n\nleaves (Go model applier): https://github.com/dmitryikh/leaves\n\nONNXMLTools (ONNX converter): https://github.com/onnx/onnxmltools\n\nSHAP (model output explainer): https://github.com/slundberg/shap\n\nShapash (model visualization and interpretation): https://github.com/MAIF/shapash\n\ndtreeviz (decision tree visualization and model interpretation): https://github.com/parrt/dtreeviz\n\nSynapseML (LightGBM on Spark): https://github.com/microsoft/SynapseML\n\nKubeflow Fairing (LightGBM on Kubernetes): https://github.com/kubeflow/fairing\n\nKubeflow Operator (LightGBM on Kubernetes): https://github.com/kubeflow/xgboost-operator\n\nlightgbm_ray (LightGBM on Ray): https://github.com/ray-project/lightgbm_ray\n\nMars (LightGBM on Mars): https://github.com/mars-project/mars\n\nML.NET (.NET/C#-package): https://github.com/dotnet/machinelearning\n\nLightGBM.NET (.NET/C#-package): https://github.com/rca22/LightGBM.Net\n\nRuby gem: https://github.com/ankane/lightgbm-ruby\n\nLightGBM4j (Java high-level binding): https://github.com/metarank/lightgbm4j\n\nlightgbm-rs (Rust binding): https://github.com/vaaaaanquish/lightgbm-rs\n\nMLflow (experiment tracking, model monitoring framework): https://github.com/mlflow/mlflow\n\n`{treesnip}` (R `{parsnip}`-compliant interface): https://github.com/curso-r/treesnip\n\n`{mlr3extralearners}` (R `{mlr3}`-compliant interface): https://github.com/mlr-org/mlr3extralearners\n\nSupport\n-------\n\n- Ask a question [on Stack Overflow with the `lightgbm` tag](https://stackoverflow.com/questions/ask?tags=lightgbm), we monitor this for new questions.\n- Open **bug reports** and **feature requests** (not questions) on [GitHub issues](https://github.com/microsoft/LightGBM/issues).\n\nHow to Contribute\n-----------------\n\nCheck [CONTRIBUTING](https://github.com/microsoft/LightGBM/blob/master/CONTRIBUTING.md) page.\n\nMicrosoft Open Source Code of Conduct\n-------------------------------------\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\nReference Papers\n----------------\n\nGuolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu. \"[LightGBM: A Highly Efficient Gradient Boosting Decision Tree](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree)\". Advances in Neural Information Processing Systems 30 (NIPS 2017), pp. 3149-3157.\n\nQi Meng, Guolin Ke, Taifeng Wang, Wei Chen, Qiwei Ye, Zhi-Ming Ma, Tie-Yan Liu. \"[A Communication-Efficient Parallel Algorithm for Decision Tree](http://papers.nips.cc/paper/6380-a-communication-efficient-parallel-algorithm-for-decision-tree)\". Advances in Neural Information Processing Systems 29 (NIPS 2016), pp. 1279-1287.\n\nHuan Zhang, Si Si and Cho-Jui Hsieh. \"[GPU Acceleration for Large-scale Tree Boosting](https://arxiv.org/abs/1706.08359)\". SysML Conference, 2018.\n\n**Note**: If you use LightGBM in your GitHub projects, please add `lightgbm` in the `requirements.txt`.\n\nLicense\n-------\n\nThis project is licensed under the terms of the MIT license. See [LICENSE](https://github.com/microsoft/LightGBM/blob/master/LICENSE) for additional details.\n"
 },
 {
  "repo": "microsoft/nni",
  "language": "Python",
  "readme_contents": "<p align=\"center\">\n<img src=\"docs/img/nni_logo.png\" width=\"300\"/>\n</p>\n\n-----------\n\n[![MIT licensed](https://img.shields.io/badge/license-MIT-brightgreen.svg)](LICENSE)\n[![Build Status](https://msrasrg.visualstudio.com/NNIOpenSource/_apis/build/status/full%20test%20-%20linux?branchName=master)](https://msrasrg.visualstudio.com/NNIOpenSource/_build/latest?definitionId=62&branchName=master)\n[![Issues](https://img.shields.io/github/issues-raw/Microsoft/nni.svg)](https://github.com/Microsoft/nni/issues?q=is%3Aissue+is%3Aopen)\n[![Bugs](https://img.shields.io/github/issues/Microsoft/nni/bug.svg)](https://github.com/Microsoft/nni/issues?q=is%3Aissue+is%3Aopen+label%3Abug)\n[![Pull Requests](https://img.shields.io/github/issues-pr-raw/Microsoft/nni.svg)](https://github.com/Microsoft/nni/pulls?q=is%3Apr+is%3Aopen)\n[![Version](https://img.shields.io/github/release/Microsoft/nni.svg)](https://github.com/Microsoft/nni/releases) [![Join the chat at https://gitter.im/Microsoft/nni](https://badges.gitter.im/Microsoft/nni.svg)](https://gitter.im/Microsoft/nni?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![Documentation Status](https://readthedocs.org/projects/nni/badge/?version=stable)](https://nni.readthedocs.io/en/stable/?badge=stable)\n\n[NNI Doc](https://nni.readthedocs.io/) | [\u7b80\u4f53\u4e2d\u6587](README_zh_CN.md)\n\n**NNI (Neural Network Intelligence)** is a lightweight but powerful toolkit to help users **automate** <a href=\"https://nni.readthedocs.io/en/stable/FeatureEngineering/Overview.html\">Feature Engineering</a>, <a href=\"https://nni.readthedocs.io/en/stable/NAS/Overview.html\">Neural Architecture Search</a>, <a href=\"https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html\">Hyperparameter Tuning</a> and <a href=\"https://nni.readthedocs.io/en/stable/Compression/Overview.html\">Model Compression</a>.\n\nThe tool manages automated machine learning (AutoML) experiments, **dispatches and runs** experiments' trial jobs generated by tuning algorithms to search the best neural architecture and/or hyper-parameters in **different training environments** like <a href=\"https://nni.readthedocs.io/en/stable/TrainingService/LocalMode.html\">Local Machine</a>, <a href=\"https://nni.readthedocs.io/en/stable/TrainingService/RemoteMachineMode.html\">Remote Servers</a>, <a href=\"https://nni.readthedocs.io/en/stable/TrainingService/PaiMode.html\">OpenPAI</a>, <a href=\"https://nni.readthedocs.io/en/stable/TrainingService/KubeflowMode.html\">Kubeflow</a>, <a href=\"https://nni.readthedocs.io/en/stable/TrainingService/FrameworkControllerMode.html\">FrameworkController on K8S (AKS etc.)</a>, <a href=\"https://nni.readthedocs.io/en/stable/TrainingService/DLTSMode.html\">DLWorkspace (aka. DLTS)</a>, <a href=\"https://nni.readthedocs.io/en/stable/TrainingService/AMLMode.html\">AML (Azure Machine Learning)</a>, <a href=\"https://nni.readthedocs.io/en/stable/TrainingService/AdaptDLMode.html\">AdaptDL (aka. ADL)</a> , other cloud options and even <a href=\"https://nni.readthedocs.io/en/stable/TrainingService/HybridMode.html\">Hybrid mode</a>.\n\n## **Who should consider using NNI**\n\n* Those who want to **try different AutoML algorithms** in their training code/model.\n* Those who want to run AutoML trial jobs **in different environments** to speed up search.\n* Researchers and data scientists who want to easily **implement and experiment new AutoML algorithms**, may it be: hyperparameter tuning algorithm, neural architect search algorithm or model compression algorithm.\n* ML Platform owners who want to **support AutoML in their platform**.\n\n## **What's NEW!** &nbsp;<a href=\"#nni-released-reminder\"><img width=\"48\" src=\"docs/img/release_icon.png\"></a>\n\n* **New release**: [v2.4 is available](https://github.com/microsoft/nni/releases) - _released on June-15-2021_\n* **New demo available**: [Youtube entry](https://www.youtube.com/channel/UCKcafm6861B2mnYhPbZHavw) | [Bilibili \u5165\u53e3](https://space.bilibili.com/1649051673) - _last updated on May-26-2021_\n* **New webinar**: [Introducing Retiarii: A deep learning exploratory-training framework on NNI](https://note.microsoft.com/MSR-Webinar-Retiarii-Registration-Live.html) - _scheduled on June-24-2021_\n* **New community channel**: [Discussions](https://github.com/microsoft/nni/discussions)\n\n## **NNI capabilities in a glance**\n\nNNI provides CommandLine Tool as well as an user friendly WebUI to manage training experiments. With the extensible API, you can customize your own AutoML algorithms and training services. To make it easy for new users, NNI also provides a set of build-in state-of-the-art AutoML algorithms and out of box support for popular training platforms.\n\nWithin the following table, we summarized the current NNI capabilities, we are gradually adding new capabilities and we'd love to have your contribution.\n\n<p align=\"center\">\n  <a href=\"#nni-has-been-released\"><img src=\"docs/img/overview.svg\" /></a>\n</p>\n\n<table>\n  <tbody>\n    <tr align=\"center\" valign=\"bottom\">\n    <td>\n      </td>\n      <td>\n        <b>Frameworks & Libraries</b>\n        <img src=\"docs/img/bar.png\"/>\n      </td>\n      <td>\n        <b>Algorithms</b>\n        <img src=\"docs/img/bar.png\"/>\n      </td>\n      <td>\n        <b>Training Services</b>\n        <img src=\"docs/img/bar.png\"/>\n      </td>\n    </tr>\n    </tr>\n    <tr valign=\"top\">\n    <td align=\"center\" valign=\"middle\">\n    <b>Built-in</b>\n      </td>\n      <td>\n      <ul><li><b>Supported Frameworks</b></li>\n        <ul>\n          <li>PyTorch</li>\n          <li>Keras</li>\n          <li>TensorFlow</li>\n          <li>MXNet</li>\n          <li>Caffe2</li>\n          <a href=\"https://nni.readthedocs.io/en/stable/SupportedFramework_Library.html\">More...</a><br/>\n        </ul>\n        </ul>\n      <ul>\n        <li><b>Supported Libraries</b></li>\n          <ul>\n           <li>Scikit-learn</li>\n           <li>XGBoost</li>\n           <li>LightGBM</li>\n           <a href=\"https://nni.readthedocs.io/en/stable/SupportedFramework_Library.html\">More...</a><br/>\n          </ul>\n      </ul>\n        <ul>\n        <li><b>Examples</b></li>\n         <ul>\n           <li><a href=\"examples/trials/mnist-pytorch\">MNIST-pytorch</li></a>\n           <li><a href=\"examples/trials/mnist-tfv1\">MNIST-tensorflow</li></a>\n           <li><a href=\"examples/trials/mnist-keras\">MNIST-keras</li></a>\n           <li><a href=\"https://nni.readthedocs.io/en/stable/TrialExample/GbdtExample.html\">Auto-gbdt</a></li>\n           <li><a href=\"https://nni.readthedocs.io/en/stable/TrialExample/Cifar10Examples.html\">Cifar10-pytorch</li></a>\n           <li><a href=\"https://nni.readthedocs.io/en/stable/TrialExample/SklearnExamples.html\">Scikit-learn</a></li>\n           <li><a href=\"https://nni.readthedocs.io/en/stable/TrialExample/EfficientNet.html\">EfficientNet</a></li>\n           <li><a href=\"https://nni.readthedocs.io/en/stable/TrialExample/OpEvoExamples.html\">Kernel Tunning</li></a>\n              <a href=\"https://nni.readthedocs.io/en/stable/SupportedFramework_Library.html\">More...</a><br/>\n          </ul>\n        </ul>\n      </td>\n      <td align=\"left\" >\n        <a href=\"https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html\">Hyperparameter Tuning</a>\n        <ul>\n          <b>Exhaustive search</b>\n          <ul>\n            <li><a href=\"https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#Random\">Random Search</a></li>\n            <li><a href=\"https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#GridSearch\">Grid Search</a></li>\n            <li><a href=\"https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#Batch\">Batch</a></li>\n            </ul>\n          <b>Heuristic search</b>\n          <ul>\n            <li><a href=\"https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#Evolution\">Na\u00efve Evolution</a></li>\n            <li><a href=\"https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#Anneal\">Anneal</a></li>\n            <li><a href=\"https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#Hyperband\">Hyperband</a></li>\n            <li><a href=\"https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#PBTTuner\">PBT</a></li>\n          </ul>\n          <b>Bayesian optimization</b>\n            <ul>\n              <li><a href=\"https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#BOHB\">BOHB</a></li>\n              <li><a href=\"https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#TPE\">TPE</a></li>\n            <li><a href=\"https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#SMAC\">SMAC</a></li>\n            <li><a href=\"https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#MetisTuner\">Metis Tuner</a></li>\n            <li><a href=\"https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#GPTuner\">GP Tuner</a></li>\n            <li><a href=\"https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#DNGOTuner\">DNGO Tuner</a></li>\n            </ul>\n        </ul>\n          <a href=\"https://nni.readthedocs.io/en/stable/NAS/Overview.html\">Neural Architecture Search (Retiarii)</a>\n          <ul>\n            <li><a href=\"https://nni.readthedocs.io/en/stable/NAS/ENAS.html\">ENAS</a></li>\n            <li><a href=\"https://nni.readthedocs.io/en/stable/NAS/DARTS.html\">DARTS</a></li>\n            <li><a href=\"https://nni.readthedocs.io/en/stable/NAS/SPOS.html\">SPOS</a></li>\n            <li><a href=\"https://nni.readthedocs.io/en/stable/NAS/Proxylessnas.html\">ProxylessNAS</a></li>\n            <li><a href=\"https://nni.readthedocs.io/en/stable/NAS/FBNet.html\">FBNet</a></li>\n            <li><a href=\"https://nni.readthedocs.io/en/stable/NAS/ExplorationStrategies.html\">Reinforcement Learning</a></li>\n            <li><a href=\"https://nni.readthedocs.io/en/stable/NAS/ExplorationStrategies.html\">Regularized Evolution</a></li>\n            <li><a href=\"https://nni.readthedocs.io/en/stable/NAS/Overview.html\">More...</a></li>\n          </ul>\n          <a href=\"https://nni.readthedocs.io/en/stable/Compression/Overview.html\">Model Compression</a>\n          <ul>\n            <b>Pruning</b>\n            <ul>\n              <li><a href=\"https://nni.readthedocs.io/en/stable/Compression/Pruner.html#agp-pruner\">AGP Pruner</a></li>\n              <li><a href=\"https://nni.readthedocs.io/en/stable/Compression/Pruner.html#slim-pruner\">Slim Pruner</a></li>\n              <li><a href=\"https://nni.readthedocs.io/en/stable/Compression/Pruner.html#fpgm-pruner\">FPGM Pruner</a></li>\n              <li><a href=\"https://nni.readthedocs.io/en/stable/Compression/Pruner.html#netadapt-pruner\">NetAdapt Pruner</a></li>\n              <li><a href=\"https://nni.readthedocs.io/en/stable/Compression/Pruner.html#simulatedannealing-pruner\">SimulatedAnnealing Pruner</a></li>\n              <li><a href=\"https://nni.readthedocs.io/en/stable/Compression/Pruner.html#admm-pruner\">ADMM Pruner</a></li>\n              <li><a href=\"https://nni.readthedocs.io/en/stable/Compression/Pruner.html#autocompress-pruner\">AutoCompress Pruner</a></li>\n              <li><a href=\"https://nni.readthedocs.io/en/stable/Compression/Overview.html\">More...</a></li>\n            </ul>\n            <b>Quantization</b>\n            <ul>\n              <li><a href=\"https://nni.readthedocs.io/en/stable/Compression/Quantizer.html#qat-quantizer\">QAT Quantizer</a></li>\n              <li><a href=\"https://nni.readthedocs.io/en/stable/Compression/Quantizer.html#dorefa-quantizer\">DoReFa Quantizer</a></li>\n              <li><a href=\"https://nni.readthedocs.io/en/stable/Compression/Quantizer.html#bnn-quantizer\">BNN Quantizer</a></li>\n            </ul>\n          </ul>\n          <a href=\"https://nni.readthedocs.io/en/stable/FeatureEngineering/Overview.html\">Feature Engineering (Beta)</a>\n          <ul>\n          <li><a href=\"https://nni.readthedocs.io/en/stable/FeatureEngineering/GradientFeatureSelector.html\">GradientFeatureSelector</a></li>\n          <li><a href=\"https://nni.readthedocs.io/en/stable/FeatureEngineering/GBDTSelector.html\">GBDTSelector</a></li>\n          </ul>\n          <a href=\"https://nni.readthedocs.io/en/stable/Assessor/BuiltinAssessor.html\">Early Stop Algorithms</a>\n          <ul>\n          <li><a href=\"https://nni.readthedocs.io/en/stable/Assessor/BuiltinAssessor.html#MedianStop\">Median Stop</a></li>\n          <li><a href=\"https://nni.readthedocs.io/en/stable/Assessor/BuiltinAssessor.html#Curvefitting\">Curve Fitting</a></li>\n          </ul>\n      </td>\n      <td>\n      <ul>\n        <li><a href=\"https://nni.readthedocs.io/en/stable/TrainingService/LocalMode.html\">Local Machine</a></li>\n        <li><a href=\"https://nni.readthedocs.io/en/stable/TrainingService/RemoteMachineMode.html\">Remote Servers</a></li>\n        <li><a href=\"https://nni.readthedocs.io/en/stable/TrainingService/HybridMode.html\">Hybrid mode</a></li>\n        <li><a href=\"https://nni.readthedocs.io/en/stable/TrainingService/AMLMode.html\">AML(Azure Machine Learning)</a></li>\n        <li><b>Kubernetes based services</b></li>\n        <ul>\n          <li><a href=\"https://nni.readthedocs.io/en/stable/TrainingService/PaiMode.html\">OpenPAI</a></li>\n          <li><a href=\"https://nni.readthedocs.io/en/stable/TrainingService/KubeflowMode.html\">Kubeflow</a></li>\n          <li><a href=\"https://nni.readthedocs.io/en/stable/TrainingService/FrameworkControllerMode.html\">FrameworkController on K8S (AKS etc.)</a></li>\n          <li><a href=\"https://nni.readthedocs.io/en/stable/TrainingService/DLTSMode.html\">DLWorkspace (aka. DLTS)</a></li>\n          <li><a href=\"https://nni.readthedocs.io/en/stable/TrainingService/AdaptDLMode.html\">AdaptDL (aka. ADL)</a></li>\n        </ul>\n      </ul>\n      </td>\n    </tr>\n      <tr align=\"center\" valign=\"bottom\">\n      </td>\n      </tr>\n      <tr valign=\"top\">\n       <td valign=\"middle\">\n    <b>References</b>\n      </td>\n     <td style=\"border-top:#FF0000 solid 0px;\">\n      <ul>\n        <li><a href=\"https://nni.readthedocs.io/en/stable/autotune_ref.html#trial\">Python API</a></li>\n        <li><a href=\"https://nni.readthedocs.io/en/stable/Tutorial/AnnotationSpec.html\">NNI Annotation</a></li>\n         <li><a href=\"https://nni.readthedocs.io/en/stable/installation.html\">Supported OS</a></li>\n      </ul>\n      </td>\n       <td style=\"border-top:#FF0000 solid 0px;\">\n      <ul>\n        <li><a href=\"https://nni.readthedocs.io/en/stable/Tuner/CustomizeTuner.html\">CustomizeTuner</a></li>\n        <li><a href=\"https://nni.readthedocs.io/en/stable/Assessor/CustomizeAssessor.html\">CustomizeAssessor</a></li>\n        <li><a href=\"https://nni.readthedocs.io/en/stable/Tutorial/InstallCustomizedAlgos.html\">Install Customized Algorithms as Builtin Tuners/Assessors/Advisors</a></li>\n        <li><a href=\"https://nni.readthedocs.io/en/stable/NAS/QuickStart.html#define-your-model-space\">Define NAS Model Space</a></li>\n        <li><a href=\"https://nni.readthedocs.io/en/stable/NAS/ApiReference.html\">NAS/Retiarii APIs</a></li>\n      </ul>\n      </td>\n        <td style=\"border-top:#FF0000 solid 0px;\">\n      <ul>\n        <li><a href=\"https://nni.readthedocs.io/en/stable/TrainingService/Overview.html\">Support TrainingService</li>\n        <li><a href=\"https://nni.readthedocs.io/en/stable/TrainingService/HowToImplementTrainingService.html\">Implement TrainingService</a></li>\n      </ul>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n## **Installation**\n\n### **Install**\n\nNNI supports and is tested on Ubuntu >= 16.04, macOS >= 10.14.1, and Windows 10 >= 1809. Simply run the following `pip install` in an environment that has `python 64-bit >= 3.6`.\n\nLinux or macOS\n\n```bash\npython3 -m pip install --upgrade nni\n```\n\nWindows\n\n```bash\npython -m pip install --upgrade nni\n```\n\nIf you want to try latest code, please [install NNI](https://nni.readthedocs.io/en/stable/installation.html) from source code.\n\nFor detail system requirements of NNI, please refer to [here](https://nni.readthedocs.io/en/stable/Tutorial/InstallationLinux.html#system-requirements) for Linux & macOS, and [here](https://nni.readthedocs.io/en/stable/Tutorial/InstallationWin.html#system-requirements) for Windows.\n\nNote:\n\n* If there is any privilege issue, add `--user` to install NNI in the user directory.\n* Currently NNI on Windows supports local, remote and pai mode. Anaconda or Miniconda is highly recommended to install [NNI on Windows](https://nni.readthedocs.io/en/stable/Tutorial/InstallationWin.html).\n* If there is any error like `Segmentation fault`, please refer to [FAQ](https://nni.readthedocs.io/en/stable/Tutorial/FAQ.html). For FAQ on Windows, please refer to [NNI on Windows](https://nni.readthedocs.io/en/stable/Tutorial/InstallationWin.html#faq).\n\n### **Verify installation**\n\n* Download the examples via clone the source code.\n\n  ```bash\n  git clone -b v2.4 https://github.com/Microsoft/nni.git\n  ```\n\n* Run the MNIST example.\n\n  Linux or macOS\n\n  ```bash\n  nnictl create --config nni/examples/trials/mnist-pytorch/config.yml\n  ```\n\n  Windows\n\n  ```powershell\n  nnictl create --config nni\\examples\\trials\\mnist-pytorch\\config_windows.yml\n  ```\n\n* Wait for the message `INFO: Successfully started experiment!` in the command line. This message indicates that your experiment has been successfully started. You can explore the experiment using the `Web UI url`.\n\n```text\nINFO: Starting restful server...\nINFO: Successfully started Restful server!\nINFO: Setting local config...\nINFO: Successfully set local config!\nINFO: Starting experiment...\nINFO: Successfully started experiment!\n-----------------------------------------------------------------------\nThe experiment id is egchD4qy\nThe Web UI urls are: http://223.255.255.1:8080   http://127.0.0.1:8080\n-----------------------------------------------------------------------\n\nYou can use these commands to get more information about the experiment\n-----------------------------------------------------------------------\n         commands                       description\n1. nnictl experiment show        show the information of experiments\n2. nnictl trial ls               list all of trial jobs\n3. nnictl top                    monitor the status of running experiments\n4. nnictl log stderr             show stderr log content\n5. nnictl log stdout             show stdout log content\n6. nnictl stop                   stop an experiment\n7. nnictl trial kill             kill a trial job by id\n8. nnictl --help                 get help information about nnictl\n-----------------------------------------------------------------------\n```\n\n* Open the `Web UI url` in your browser, you can view detailed information of the experiment and all the submitted trial jobs as shown below. [Here](https://nni.readthedocs.io/en/stable/Tutorial/WebUI.html) are more Web UI pages.\n\n<img src=\"docs/static/img/webui.gif\" alt=\"webui\" width=\"100%\"/>\n\n## **Releases and Contributing**\nNNI has a monthly release cycle (major releases). Please let us know if you encounter a bug by [filling an issue](https://github.com/microsoft/nni/issues/new/choose).\n\nWe appreciate all contributions. If you are planning to contribute any bug-fixes, please do so without further discussions.\n\nIf you plan to contribute new features, new tuners, new training services, etc. please first open an issue or reuse an exisiting issue, and discuss the feature with us. We will discuss with you on the issue timely or set up conference calls if needed.\n\nTo learn more about making a contribution to NNI, please refer to our [How-to contribution page](https://nni.readthedocs.io/en/stable/contribution.html). \n\nWe appreciate all contributions and thank all the contributors!\n\n<a href=\"https://github.com/microsoft/nni/graphs/contributors\"><img src=\"docs/img/contributors.png\" /></a>\n\n\n## **Feedback**\n* [File an issue](https://github.com/microsoft/nni/issues/new/choose) on GitHub.\n* Open or participate in a [discussion](https://github.com/microsoft/nni/discussions). \n* Discuss on the NNI [Gitter](https://gitter.im/Microsoft/nni?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) in NNI.\n\nJoin IM discussion groups:\n|Gitter||WeChat|\n|----|----|----|\n|![image](https://user-images.githubusercontent.com/39592018/80665738-e0574a80-8acc-11ea-91bc-0836dc4cbf89.png)| OR |![image](https://github.com/scarlett2018/nniutil/raw/master/wechat.png)|\n\n\n## Test status\n\n### Essentials\n\n| Type | Status |\n| :---: | :---: |\n| Fast test | [![Build Status](https://msrasrg.visualstudio.com/NNIOpenSource/_apis/build/status/fast%20test?branchName=master)](https://msrasrg.visualstudio.com/NNIOpenSource/_build/latest?definitionId=54&branchName=master) |\n| Full linux | [![Build Status](https://msrasrg.visualstudio.com/NNIOpenSource/_apis/build/status/full%20test%20-%20linux?repoName=microsoft%2Fnni&branchName=master)](https://msrasrg.visualstudio.com/NNIOpenSource/_build/latest?definitionId=62&repoName=microsoft%2Fnni&branchName=master) |\n| Full windows | [![Build Status](https://msrasrg.visualstudio.com/NNIOpenSource/_apis/build/status/full%20test%20-%20windows?branchName=master)](https://msrasrg.visualstudio.com/NNIOpenSource/_build/latest?definitionId=63&branchName=master) |\n\n### Training services\n\n| Type | Status |\n| :---: | :---: |\n| Remote - linux to linux | [![Build Status](https://msrasrg.visualstudio.com/NNIOpenSource/_apis/build/status/integration%20test%20-%20remote%20-%20linux%20to%20linux?branchName=master)](https://msrasrg.visualstudio.com/NNIOpenSource/_build/latest?definitionId=64&branchName=master) |\n| Remote - linux to windows | [![Build Status](https://msrasrg.visualstudio.com/NNIOpenSource/_apis/build/status/integration%20test%20-%20remote%20-%20linux%20to%20windows?branchName=master)](https://msrasrg.visualstudio.com/NNIOpenSource/_build/latest?definitionId=67&branchName=master) |\n| Remote - windows to linux | [![Build Status](https://msrasrg.visualstudio.com/NNIOpenSource/_apis/build/status/integration%20test%20-%20remote%20-%20windows%20to%20linux?branchName=master)](https://msrasrg.visualstudio.com/NNIOpenSource/_build/latest?definitionId=68&branchName=master) |\n| OpenPAI | [![Build Status](https://msrasrg.visualstudio.com/NNIOpenSource/_apis/build/status/integration%20test%20-%20openpai%20-%20linux?branchName=master)](https://msrasrg.visualstudio.com/NNIOpenSource/_build/latest?definitionId=65&branchName=master) |\n| Frameworkcontroller | [![Build Status](https://msrasrg.visualstudio.com/NNIOpenSource/_apis/build/status/integration%20test%20-%20frameworkcontroller?branchName=master)](https://msrasrg.visualstudio.com/NNIOpenSource/_build/latest?definitionId=70&branchName=master) |\n| Kubeflow | [![Build Status](https://msrasrg.visualstudio.com/NNIOpenSource/_apis/build/status/integration%20test%20-%20kubeflow?branchName=master)](https://msrasrg.visualstudio.com/NNIOpenSource/_build/latest?definitionId=69&branchName=master) |\n| Hybrid | [![Build Status](https://msrasrg.visualstudio.com/NNIOpenSource/_apis/build/status/integration%20test%20-%20hybrid?branchName=master)](https://msrasrg.visualstudio.com/NNIOpenSource/_build/latest?definitionId=79&branchName=master) |\n| AzureML | [![Build Status](https://msrasrg.visualstudio.com/NNIOpenSource/_apis/build/status/integration%20test%20-%20aml?branchName=master)](https://msrasrg.visualstudio.com/NNIOpenSource/_build/latest?definitionId=78&branchName=master) |\n\n## Related Projects\n\nTargeting at openness and advancing state-of-art technology, [Microsoft Research (MSR)](https://www.microsoft.com/en-us/research/group/systems-and-networking-research-group-asia/) had also released few other open source projects.\n\n* [OpenPAI](https://github.com/Microsoft/pai) : an open source platform that provides complete AI model training and resource management capabilities, it is easy to extend and supports on-premise, cloud and hybrid environments in various scale.\n* [FrameworkController](https://github.com/Microsoft/frameworkcontroller) : an open source general-purpose Kubernetes Pod Controller that orchestrate all kinds of applications on Kubernetes by a single controller.\n* [MMdnn](https://github.com/Microsoft/MMdnn) : A comprehensive, cross-framework solution to convert, visualize and diagnose deep neural network models. The \"MM\" in MMdnn stands for model management and \"dnn\" is an acronym for deep neural network.\n* [SPTAG](https://github.com/Microsoft/SPTAG) : Space Partition Tree And Graph (SPTAG) is an open source library for large scale vector approximate nearest neighbor search scenario.\n* [nn-Meter](https://github.com/microsoft/nn-Meter) : An accurate inference latency predictor for DNN models on diverse edge devices.\n\nWe encourage researchers and students leverage these projects to accelerate the AI development and research.\n\n## **License**\n\nThe entire codebase is under [MIT license](LICENSE)\n"
 }
]